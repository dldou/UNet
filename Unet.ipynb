{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebec0061-0aa9-497d-b3f7-a1eb8b22ba0e",
   "metadata": {},
   "source": [
    "# U-Net \n",
    "Version of the scientific paper:   \n",
    "O. Ronneberger and P.Fischer and T. Brox, 2015, U-Net: Convolutional Networks for Biomedical Image Segmentation, Medical Image Computing and Computer-Assisted Intervention (MICCAI). \n",
    "\n",
    "**Our goal is to segment the food elements in each image from Tray Dataset.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c51701d-7641-4bf6-b574-cd28637900c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics as met\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c2f30e-e8b6-4a0a-be6d-caf3c4b96d03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data\n",
    "\n",
    "The data need a layout:\n",
    "- need to reshape to 572x572\n",
    "- need to create a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c2f6fa0-177a-408b-88d9-57d8cb04c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb9a40c9-362f-479a-81c4-e1d8d5255b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the Dataset class\n",
    "class TrayDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file_path, \n",
    "                 _img_dir_path, _label_dir_path,\n",
    "                 _transforms = None, _target_transforms = None):\n",
    "        super(TrayDataset, self).__init__()\n",
    "\n",
    "        self.labels            = pd.read_csv(annotations_file_path)\n",
    "        self.img_dir_path      = _img_dir_path\n",
    "        self.label_dir_path    = _label_dir_path\n",
    "        self.transforms        = _transforms\n",
    "        self.target_transforms = _target_transforms\n",
    "\n",
    "\n",
    "    #Must be the nof samples\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir_path))\n",
    "\n",
    "\n",
    "    def __getitem__(self, img_idx):\n",
    "\n",
    "        #/!\\ NOT THAT GOOD BECAUSE DATA LIST ARE SORTED EVERY TIME WE ACCESS#\n",
    "        img_name   = sorted(os.listdir(self.img_dir_path))[img_idx]\n",
    "        label_name = sorted(os.listdir(self.label_dir_path))[img_idx]\n",
    "\n",
    "        #Get the image\n",
    "        image_path = self.img_dir_path + img_name\n",
    "        image      = Image.open(image_path)\n",
    "        if self.transforms:\n",
    "            image  = self.transforms(image)\n",
    "        #Get the label\n",
    "        label_path = self.label_dir_path + label_name\n",
    "        label      = Image.open(label_path)\n",
    "        if self.target_transforms:\n",
    "            label  = self.target_transforms(label)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "828fb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to transform Masks (targets) into a Tensor without the unwanted scaling\n",
    "class MaskPILToTensor:\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = functional.pil_to_tensor(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7229d8-5a44-4874-87bb-bd0edb30fab6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc02b204-53e7-4257-a51f-31d47772b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5554898e-9e66-4de0-a21f-69f290eb0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_x2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, ker_size=3):\n",
    "\n",
    "        super(Conv_x2, self).__init__()\n",
    "\n",
    "        self.conv_x2 = nn.Sequential (\n",
    "            nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=ker_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_ch, out_channels=out_ch, kernel_size=ker_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_x2(x)\n",
    "\n",
    "\n",
    "class Block_down_unet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, ker_size=3):\n",
    "\n",
    "        super(Block_down_unet, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential (\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            Conv_x2(in_ch, out_ch, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "\n",
    "class Block_up_unet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, ker_size=3):\n",
    "\n",
    "        super(Block_up_unet, self).__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_channels=in_ch, out_channels=(in_ch//2), kernel_size=2, stride=2)\n",
    "        self.conv_x2 = Conv_x2(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, concat_tensor, processed_tensor):\n",
    "        \n",
    "        #Upsampling\n",
    "        processed_tensor = self.up(processed_tensor)\n",
    "\n",
    "        #Padding and concatenation path\n",
    "        diffY = concat_tensor.size()[2] - processed_tensor.size()[2]\n",
    "        diffX = concat_tensor.size()[3] - processed_tensor.size()[3]\n",
    "\n",
    "        x = F.pad(processed_tensor, [diffX // 2, diffX - diffX // 2,\n",
    "                                     diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([concat_tensor, x], dim=1)\n",
    "\n",
    "        #Double convolution\n",
    "        x = self.conv_x2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.nof_channels = in_channels\n",
    "        self.nof_classes  = n_classes\n",
    "\n",
    "        #First layer\n",
    "        self.first_layer = Conv_x2(self.nof_channels, 64)\n",
    "\n",
    "        #Contracting path\n",
    "        self.down1 = Block_down_unet(64, 128, 3)\n",
    "        self.down2 = Block_down_unet(128, 256, 3)\n",
    "        self.down3 = Block_down_unet(256, 512, 3)\n",
    "\n",
    "        #Bottom layer\n",
    "        self.down4 = Block_down_unet(512, 1024, 3)\n",
    "\n",
    "        #Expanding path\n",
    "        self.up1 = Block_up_unet(1024, 512, 3)\n",
    "        self.up2 = Block_up_unet(512, 256, 3)\n",
    "        self.up3 = Block_up_unet(256, 128, 3)\n",
    "        self.up4 = Block_up_unet(128, 64, 3)\n",
    "\n",
    "        #Last block with 1x1 convolution\n",
    "        self.last_block = nn.Conv2d(in_channels=64, out_channels=n_classes, kernel_size=1)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        #Encoder (contracting path)\n",
    "        x1 = self.first_layer(X)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "\n",
    "        #Bottom layer\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        #Decode (expanding path + concatenating path)\n",
    "        x = self.up1(x4, x5)\n",
    "        x = self.up2(x3, x)\n",
    "        x = self.up3(x2, x)\n",
    "        x = self.up4(x1, x)\n",
    "\n",
    "        #Last layer (unomalized logits (sum != 1 and can be negatives))\n",
    "        logits = self.last_block(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db5064a4",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3822e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, optimizer, loss_fn, metric_fn, device):\n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = loss_fn\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        if next(model.parameters()).device != self.device:\n",
    "            self.model = model.to(self.device)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "        if metric_fn.device != self.device:\n",
    "            self.metric = metric_fn.to(self.device)\n",
    "\n",
    "    def get_train_dataloader(self, **kwargs) -> DataLoader:\n",
    "\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        if 'batch_size' in kwargs:\n",
    "            _batch_size = kwargs.get('batch_size')\n",
    "        else:\n",
    "            _batch_size = 32\n",
    "            \n",
    "        if 'shuffle' in kwargs:\n",
    "            _shuffle = kwargs.get('shuffle')\n",
    "        else:\n",
    "            _shuffle = True\n",
    "\n",
    "        return DataLoader(self.train_dataset, shuffle=_shuffle, batch_size=_batch_size)\n",
    "\n",
    "\n",
    "    def get_test_dataloader(self, **kwargs) -> DataLoader:\n",
    "\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: testing requires a test_dataset.\")\n",
    "\n",
    "        if 'batch_size' in kwargs:\n",
    "            _batch_size = kwargs.get('batch_size')\n",
    "        else:\n",
    "            _batch_size = 32\n",
    "        if 'shuffle' in kwargs:\n",
    "            _shuffle = kwargs.get('shuffle')\n",
    "        else:\n",
    "            _shuffle = True\n",
    "\n",
    "        return DataLoader(self.test_dataset, shuffle=_shuffle, batch_size=_batch_size)\n",
    "\n",
    "\n",
    "    def train_step(self, train_loader):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        self.model.train()\n",
    "\n",
    "        for data in train_loader:\n",
    "\n",
    "                #Data send to device + requires_grad=True\n",
    "                images, targets = data[0].to(self.device), data[1].to(self.device)\n",
    "                #Zero the gradient\n",
    "                self.optimizer.zero_grad()\n",
    "                #Predictions \n",
    "                outputs = self.model(images)\n",
    "                #Cropping (crop imgs --> easier to process)\n",
    "                crop_image = transforms.CenterCrop(outputs.size()[3])\n",
    "                targets    = crop_image(targets)\n",
    "                #Loss\n",
    "                _, targets = torch.max(targets, dim=1)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                #Compute gradient\n",
    "                loss.backward()\n",
    "                #Update parameters of the model\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "\n",
    "        return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "    def test_step(self, test_loader):\n",
    "\n",
    "        running_loss   = 0.0\n",
    "        accuracy        = 0.0\n",
    "        nof_predictions = 0.0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        #Fasten the inference by setting every requires_grad to False\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for data in test_loader:\n",
    "                #Get data and send them to the device\n",
    "                images, targets = data[0].to(self.device), data[1].to(self.device) \n",
    "                #Run the model on the test set\n",
    "                outputs = self.model(images)\n",
    "                #Cropping the image /!\\ /!\\ MUST GO /!\\ /!\\\n",
    "                crop_image = transforms.CenterCrop(outputs.size()[3])\n",
    "                targets     = crop_image(targets)\n",
    "                #Compute the loss on the batch\n",
    "                _, targets = torch.max(targets, dim=1)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                running_loss += loss.item()  \n",
    "                # Compute the Jaccard Multiclass index\n",
    "                _, target = torch.max(targets, dim=0)\n",
    "                self.metric.update(outputs, targets)\n",
    "\n",
    "        accuracy = self.metric.compute()\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def train(self, train_loader, test_loader, nof_epochs, \n",
    "              file_path_save_model, save_epoch_path, results_file_path,\n",
    "              train_loss_name, accuracy_name, test_loss_name,\n",
    "              best_accuracy_is_maximal = False):\n",
    "        \"\"\"\n",
    "            Main training entry point.\n",
    "        \"\"\"\n",
    "        print(\"Starting training...\\n\")\n",
    "        print(\"The model will be running on\", next(self.model.parameters()).device, \"device.\\n\")\n",
    "        \n",
    "        best_accuracy = 0.0\n",
    "        results = []\n",
    "\n",
    "        for epoch in range(1, nof_epochs+1):\n",
    "\n",
    "            epoch_accuracy   = 0.0\n",
    "            train_epoch_loss = 0.0\n",
    "            test_epoch_loss  = 0.0\n",
    "\n",
    "            #Training\n",
    "            train_epoch_loss = self.train_step(train_loader)\n",
    "            #Validation\n",
    "            epoch_accuracy   = self.test_step(test_loader)\n",
    "        \n",
    "            print(f'Epoch: {epoch}, {train_loss_name}: {train_epoch_loss}, {accuracy_name}: {epoch_accuracy}')\n",
    "            \n",
    "            #Save model when best accuracy is beaten\n",
    "            if best_accuracy_is_maximal:\n",
    "                if epoch_accuracy > best_accuracy:\n",
    "                    save_epoch_path = save_epoch_path\n",
    "                    self.save_model(save_epoch_path)\n",
    "                    best_accuracy = epoch_accuracy\n",
    "            else:\n",
    "                if epoch_accuracy < best_accuracy:\n",
    "                    save_epoch_path = save_epoch_path\n",
    "                    self.save_model(save_epoch_path)\n",
    "                    best_accuracy = epoch_accuracy\n",
    "\n",
    "            results.append((epoch, train_epoch_loss, epoch_accuracy))        \n",
    "\n",
    "        # Saving the model\n",
    "        print('Saving the model...\\n')\n",
    "        self.model = self.model.to('cpu')\n",
    "        self.save_model(file_path_save_model)\n",
    "\n",
    "        # Saving the performances\n",
    "        with open(results_file_path, 'wb') as f:\n",
    "            pkl.dump(results, f) \n",
    "\n",
    "        print(\"Training finish.\\n\") \n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"\n",
    "        Function to save model's parameters\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), file_path)\n",
    "\n",
    "\n",
    "    def load_model(self, file_path, load_on_GPU=True):\n",
    "        \"\"\"\n",
    "            Function to load function when only the params have been saved\n",
    "        \"\"\"\n",
    "        if load_on_GPU:\n",
    "            params = torch.load(file_path, map_location='cuda:0')\n",
    "            self.model.load_state_dict(params, map_location='cuda:0')\n",
    "        else:\n",
    "            params = torch.load(file_path, map_location='cpu')\n",
    "            self.model.load_state_dict(params, map_location='cpu')\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, epoch, file_path):\n",
    "        \"\"\"\n",
    "            Function to save model's checkpoints\n",
    "        \"\"\"\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'loss': self.loss}, \n",
    "                    file_path)\n",
    "\n",
    "\n",
    "    def load_checkpoint(self, file_path, load_on_GPU=True):\n",
    "        \"\"\"\n",
    "            Load checkpoints\n",
    "        \"\"\"\n",
    "        if load_on_GPU:\n",
    "            checkpoint = torch.load(file_path, map_location='cuda:0')\n",
    "        else:\n",
    "            checkpoint = torch.load(file_path, map_location='cpu')\n",
    "\n",
    "        #Loading\n",
    "        if load_on_GPU:\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'], map_location='cuda:0')\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'], map_location='cuda:0')\n",
    "        else:\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'], map_location='cpu')\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'], map_location='cpu')\n",
    "        \n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "\n",
    "        return epoch, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343ce38-4ea0-448b-9ff7-ab5b36f35100",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46f23d0b-09f3-4301-89a2-dd2fd927818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d2c074a-a8bf-4d05-bcdf-42c8e64c4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model, test_loader):\n",
    "    \"\"\"\n",
    "    Display few images and segmentations\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    #Display 4*4=16 images\n",
    "    for i in range(16 // 2):\n",
    "\n",
    "        #Select a random image in the dataset\n",
    "        nof_images = len(test_loader.dataset)\n",
    "        idx = random.randrange(nof_images)\n",
    "        #Inference\n",
    "        image = test_loader.dataset[idx][0].unsqueeze(0).to('cuda:0')\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            segmented_image = model(image)\n",
    "\n",
    "        #Sent back the image to the CPU\n",
    "        image = image.squeeze().to('cpu').permute(1,2,0)\n",
    "        segmented_image = segmented_image.squeeze().to('cpu').permute(1,2,0)\n",
    "        print(segmented_image.shape)\n",
    "        print(image.shape)\n",
    "\n",
    "        #Plot\n",
    "        # ax_image = plt.subplot(4,4, 2*i+1 )\n",
    "        # ax_image.set_title(\"Real image\")\n",
    "        # plt.imshow(image, cmap='gray_r')\n",
    "        # plt.axis('off')\n",
    "\n",
    "        # ax_segmented_image = plt.subplot(4,4, 2*(i+1) )\n",
    "        # ax_segmented_image.set_title(\"Segmented image\")\n",
    "        # plt.imshow(segmented_image, cmap='gray_r')\n",
    "        # plt.axis('off')\n",
    "    \n",
    "    #fig.suptitle(\"{} on few examples\\n Reached accuracy with 15 epochs: X%\".format(model.__class__.__name__))\n",
    "\n",
    "    #Save\n",
    "    #plt.savefig(str(model.__class__.__name__) + \"_accuraccy_X\" + \".pdf\")\n",
    "\n",
    "    #Show\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c832d9-6526-4cc0-84a1-818abaee6b25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93109007-6e86-47a4-ad9d-68196be90bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torchmetrics import JaccardIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40224c3a-9e35-4cf4-a320-2b07136e288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "The model will be running on cuda:0 device.\n",
      "\n",
      "Epoch: 1, Cross Entropy Loss 2D: 0.12604233931300543, Jaccard score: 0.023255813866853714\n",
      "Epoch: 2, Cross Entropy Loss 2D: 3.3099767447113306e-06, Jaccard score: 0.023255813866853714\n",
      "Epoch: 3, Cross Entropy Loss 2D: 9.890742164781912e-07, Jaccard score: 0.023255813866853714\n",
      "Saving the model...\n",
      "\n",
      "Training finish.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    ### Data\n",
    "    transform        = transforms.Compose([transforms.Pad((78,158), fill=0, padding_mode='constant'),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize(mean=0.5, std=0.5)])\n",
    "    \n",
    "    target_transform = transforms.Compose([transforms.Pad((78,158), fill=0, padding_mode='constant'),\n",
    "                                           MaskPILToTensor()])\n",
    "\n",
    "    annotations_file_path    = '/home/dldou/Projets_ML/Unet/classes.csv'\n",
    "    train_dataset_img_path   = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/XTrain/'\n",
    "    train_dataset_label_path = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/yTrain/'\n",
    "  \n",
    "    train_dataset            = TrayDataset(annotations_file_path, \n",
    "                                           train_dataset_img_path, train_dataset_label_path,\n",
    "                                           transform, target_transform\n",
    "                                           )\n",
    "\n",
    "    test_dataset_img_path   = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/XTest/'\n",
    "    test_dataset_label_path = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/yTest/'    \n",
    "\n",
    "    test_dataset            = TrayDataset(annotations_file_path, \n",
    "                                           test_dataset_img_path, test_dataset_label_path,\n",
    "                                           transform, target_transform\n",
    "                                           )\n",
    "    \n",
    "\n",
    "    ### Model \n",
    "    in_channels = 3 \n",
    "    n_classes = len(train_dataset.labels) \n",
    "    Unet = UNet(in_channels, n_classes)\n",
    "    #summary(Unet, (3, 572, 572))\n",
    "\n",
    "    ### Trainer \n",
    "    # Hyper-params\n",
    "    lr          = 1e-4\n",
    "    _batch_size = 2\n",
    "    n_epochs    = 3\n",
    "\n",
    "    # Other params\n",
    "    model = Unet\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    metric_fn = met.JaccardIndex(task=\"multiclass\", num_classes=n_classes)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    trainer = Trainer(model, train_dataset, test_dataset, optimizer, loss_fn, metric_fn, device)\n",
    "\n",
    "    # Dataloader\n",
    "    train_dataloader = trainer.get_train_dataloader(shuffle=True, batch_size=_batch_size)\n",
    "    test_dataloader  = trainer.get_test_dataloader(shuffle=True, batch_size=_batch_size)\n",
    "\n",
    "    # Training\n",
    "    file_path_save_model = \"./saving/trained_Unet_model.pth\"\n",
    "    save_epoch_path = \"./saving/trained_Unet_model_best_accuracy.pth\"    \n",
    "    results_file_path = \"results.pkl\"\n",
    "    train_loss_name = \"Cross Entropy Loss 2D\"\n",
    "    accuracy_name = \"Jaccard score\"\n",
    "    test_loss_name = train_loss_name\n",
    "    best_accuracy_is_maximal = True\n",
    "\n",
    "    trainer.train(train_dataloader, test_dataloader, n_epochs, \n",
    "                  file_path_save_model, save_epoch_path, results_file_path,\n",
    "                  train_loss_name, accuracy_name, test_loss_name,\n",
    "                  best_accuracy_is_maximal)\n",
    "    \n",
    "\n",
    "    ### Inferences and performances\n",
    "    #plot_results(Unet, test_dataloader, device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8cc5b7cf7ab45a4eb9c5d10fcde61976ab495d4e3d71a8f87de6440d779c3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
