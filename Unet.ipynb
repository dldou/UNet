{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebec0061-0aa9-497d-b3f7-a1eb8b22ba0e",
   "metadata": {},
   "source": [
    "# Segmentation task using U-Net\n",
    "Version of the scientific paper:   \n",
    "O. Ronneberger and P.Fischer and T. Brox, 2015, U-Net: Convolutional Networks for Biomedical Image Segmentation, Medical Image Computing and Computer-Assisted Intervention (MICCAI). \n",
    "\n",
    "**Our goal is to segment the food elements in the images from the Tray Dataset.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c51701d-7641-4bf6-b574-cd28637900c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics as met\n",
    "import torchvision.transforms.functional as functional\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5c2f30e-e8b6-4a0a-be6d-caf3c4b96d03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data\n",
    "\n",
    "The data come from the Tray Dataset available on Kaggle. **PUT REFERENCES**  \n",
    "Statistical analysis on the train subset.   \n",
    "\n",
    "During our analysis of the images we find out that the intensity of the pixels isn't following a normal distribution and has a very long \"tail\" in each channel. However, the distribution seems similar for each channels. This might be a problem for the segmentation task of the pixels. We'll have to preprocess the images with appropriate transformations to overcome this.\n",
    "\n",
    "As we can see from the various plot, the frequency of labels in images varies very much. Even some classes aren't represetend at all in the train dataset: straw, beef, meetball, ... Those classes have to be removed from the dataset in order to save up the compute ressources and considering the **no-free lunch theorem (give the reference here)**.  \n",
    "For the other classes, we observe that the repartition of the first three classes (backgroung, tray, clutlery) is homogeneous but they are far more represented in the dataset's labels than any other one. Moreover, some other classes are absent from some images and present a normal distribution of the number of pixels in the rest.    \n",
    "This may cause a problem when using the cross entropy loss function. **tell mathematically why here**.   \n",
    "  \n",
    "From our analysis, we'll delete the following classes from our considerations: from, straw, meatball, beef, carot, celery, water.   \n",
    "Moreover, to facilitate the problem, we'll try to predict the first 3 classes to start: background, tray, cutlery. The strategy we'd like to apply is to determine the frequency by considering only those three and maybe apply weights on the loss to correct the unbalanced representation.  \n",
    "\n",
    "One of the problems we have to deal with is by which class the rest of the set of the class should be replace?\n",
    "Because the sum of all of the pixels labelled with one of the rest of the classes represents a fraction of almost 10% of the total of pixels, we'll create a class tagged as 'undetermined'.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2f6fa0-177a-408b-88d9-57d8cb04c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82ca074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskPILToTensor:\n",
    "    \"\"\"\n",
    "        Class to transform Masks (targets) into a Tensor without the unwanted scaling of ToTensor\n",
    "    \"\"\"\n",
    "    def __call__(self, mask):\n",
    "        mask = functional.pil_to_tensor(mask)\n",
    "        return mask\n",
    "    \n",
    "class MaskTensorToPIL:\n",
    "    \"\"\"\n",
    "        Inverse transformation of MaskToPIL\n",
    "    \"\"\"\n",
    "    def __call__(self, mask):\n",
    "        mask = functional.to_pil_image(mask)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb9a40c9-362f-479a-81c4-e1d8d5255b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrayDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file_path, _img_dir_path, _label_dir_path,\n",
    "                 scal_norm_transform=None, random_rot_transform=None, mask_transform=MaskPILToTensor(),\n",
    "                 data_augmentation=False, \n",
    "                 adapt_to_task=False, first_class_idx_of_the_rest=None):\n",
    "        print(\"Initialization of TrayDataset...\")\n",
    "        super(TrayDataset, self).__init__()\n",
    "        self.classes_dataframe    = pd.read_csv(annotations_file_path)\n",
    "        self.img_dir_path         = _img_dir_path\n",
    "        self.label_dir_path       = _label_dir_path\n",
    "        self.scal_norm_transform  = scal_norm_transform\n",
    "        self.random_rot_transform = random_rot_transform\n",
    "        self.mask_transform       = mask_transform\n",
    "        self.data_augmentation    = data_augmentation\n",
    "\n",
    "        self.img_filenames   = sorted(os.listdir(self.img_dir_path))\n",
    "        self.label_filenames = sorted(os.listdir(self.label_dir_path))\n",
    "\n",
    "        # adaptation of the labels and the classes dataframe to the task\n",
    "        if adapt_to_task and first_class_idx_of_the_rest is not None:\n",
    "            print(\"Adapting the labels and dataframe to task...\")\n",
    "            self.first_class_idx_of_the_rest = first_class_idx_of_the_rest\n",
    "            new_label_dir_path = self.__adapt_labels_and_dataframe_to_task()\n",
    "            self.label_dir_path = new_label_dir_path\n",
    "        else:\n",
    "            self.first_class_idx_of_the_rest = None\n",
    "            \n",
    "        print(\"Init TrayDataset done.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.img_dir_path))\n",
    "    \n",
    "\n",
    "    def __adapt_labels_and_dataframe_to_task(self):\n",
    "        \"\"\"\n",
    "            Modify the ground truth labels and the classes dataframe by regrouping all the classes which index\n",
    "            is above or equal to first_index_of_the_rest under one big class ('undetermined')\n",
    "            -> must be private and use in the init\n",
    "        \"\"\"\n",
    "        # modify the mask \n",
    "        for label_idx in range(len(os.listdir(self.label_dir_path))):\n",
    "            # label is a tensor thanks to __getitem__\n",
    "            _, label = self.__getitem__(label_idx)\n",
    "            # replace all the class with index > first_class_idx_of_the_rest by first_class_idx_of_the_rest\n",
    "            label[label > self.first_class_idx_of_the_rest] = self.first_class_idx_of_the_rest \n",
    "            # save label in new dir\n",
    "            if label_idx == 0:\n",
    "                new_dir_path = self.__save_modified_labels(label_idx, label)\n",
    "            else:\n",
    "                _ = self.__save_modified_labels(label_idx, label)\n",
    "\n",
    "        # drop rows of the rest of the class\n",
    "        self.classes_dataframe.drop(self.classes_dataframe[self.classes_dataframe['_id'] >= self.first_class_idx_of_the_rest].index, \n",
    "                                    inplace=True\n",
    "                                    )\n",
    "        # create a new row that regroups all the dropped classes into one undetermined class\n",
    "        new_class_row = pd.DataFrame({'_id': self.first_class_idx_of_the_rest, '_class': 'undeterminded', '_name': 'undetermined'}, \n",
    "                                     index=[len(self.classes_dataframe)]\n",
    "                                     )\n",
    "        self.classes_dataframe = pd.concat([self.classes_dataframe.loc[:], new_class_row])\n",
    "        print(\"Adapt TrayDataset to the task done.\")\n",
    "\n",
    "        return new_dir_path\n",
    "\n",
    "\n",
    "    def __save_modified_labels(self, label_idx, label):\n",
    "        \"\"\"\n",
    "            Save the labels as PNG images to the specified directory.\n",
    "\n",
    "            Args:\n",
    "                labels (list of numpy.ndarray): The modified labels to save.\n",
    "                directory (str): The directory to save the labels to.\n",
    "        \"\"\"\n",
    "        # inverse transform applied on labels\n",
    "        transform = MaskTensorToPIL()\n",
    "        # create the directory to store the modified labels if it doesn't exist\n",
    "        new_yTrain_dir_name = \"./TrayDataset/TrayDataset/yTrain_adapted_to_the_task\"\n",
    "        if not os.path.exists(new_yTrain_dir_name):\n",
    "            os.makedirs(new_yTrain_dir_name)\n",
    "\n",
    "        # save label as PNG image\n",
    "        label_png = transform(label)\n",
    "        label_name = self.label_filenames[label_idx]\n",
    "        file_path = os.path.join(new_yTrain_dir_name, f'{label_name}')\n",
    "        label_png.save(file_path)\n",
    "        \n",
    "        return new_yTrain_dir_name\n",
    "\n",
    "\n",
    "    def transform_fn(self, image, label):\n",
    "        \"\"\"\n",
    "            Transforms pipeline\n",
    "        \"\"\"\n",
    "        # data augmentation\n",
    "        if self.data_augmentation:\n",
    "            # horizontal flipping\n",
    "            if random.random() > 0.5:\n",
    "                image = transforms.functional.hflip(image)\n",
    "                label = transforms.functional.hflip(label)\n",
    "            # vertical flipping\n",
    "            if random.random() > 0.5:\n",
    "                image = transforms.functional.vflip(image)\n",
    "                label = transforms.functional.vflip(label)\n",
    "\n",
    "        # random rotation (concatenation in order to obtain the same rotation)\n",
    "        # at this point image and label are still in (height, width, channel) layout\n",
    "        if self.random_rot_transform is not None:\n",
    "            concat_img_label   = torch.cat((image, label), 0)\n",
    "            transformed_concat = transforms.RandomRotation(10)(concat_img_label)\n",
    "\n",
    "            image = transformed_concat[0]\n",
    "            label = transformed_concat[1]\n",
    "\n",
    "        # scal and normalization\n",
    "        if self.scal_norm_transform is not None:\n",
    "            image = self.scal_norm_transform(image)\n",
    "\n",
    "        if self.mask_transform is not None:\n",
    "            label = self.mask_transform(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "    def __getitem__(self, img_idx):\n",
    "        img_name   = self.img_filenames[img_idx]\n",
    "        label_name = self.label_filenames[img_idx]\n",
    "\n",
    "        image_path = os.path.join(self.img_dir_path, img_name)\n",
    "        image      = Image.open(image_path)\n",
    "        label_path = os.path.join(self.label_dir_path, label_name)\n",
    "        label      = Image.open(label_path)\n",
    "        \n",
    "        image, label = self.transform_fn(image, label)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "    def get_mean_std_images(self):\n",
    "        \"\"\"\n",
    "            Function to obtain the mean and std of a dataset for each channels \n",
    "            after a ToTensor transform with scaling\n",
    "        \"\"\"\n",
    "        images_tens = []\n",
    "        for image_idx in range(len(os.listdir(self.img_dir_path))):\n",
    "            image, _ = self.__getitem__(image_idx)\n",
    "            images_tens.append(image)\n",
    "        images_tens = torch.stack(images_tens)\n",
    "\n",
    "        mean = torch.mean(images_tens, dim=(0,2,3))\n",
    "        std = torch.std(images_tens, dim=(0,2,3))\n",
    "\n",
    "        return [mean[i].item() for i in range(3)], [std[i].item() for i in range(3)]\n",
    "    \n",
    "\n",
    "    def get_classes_frequency(self):\n",
    "        \"\"\"\n",
    "            Used to obtain the frequency of each classes in the ground truth labels after all the transformations have been applied\n",
    "            on the labels \n",
    "            (Random flips doesn't change the representation of classes in the set)\n",
    "            (except doesn't suit Random Rotation)\n",
    "        \"\"\"\n",
    "        classes_frequency = torch.zeros(len(self.classes_dataframe))\n",
    "\n",
    "        n_labels          = len(os.listdir(self.label_dir_path))\n",
    "        for label_idx in range(n_labels):\n",
    "            _, label = self.__getitem__(label_idx)\n",
    "            for class_idx in range(self.first_class_idx_of_the_rest + 1):\n",
    "                classes_frequency[class_idx] += torch.count_nonzero(label == class_idx).item()\n",
    "\n",
    "        return classes_frequency / classes_frequency.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a5547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset, **kwargs) -> DataLoader:\n",
    "     \n",
    "    if dataset is None:\n",
    "        raise ValueError(\"Dataset object must not be None.\")\n",
    "\n",
    "    if 'batch_size' in kwargs:\n",
    "            _batch_size = kwargs.get('batch_size')\n",
    "    else:\n",
    "        _batch_size = 32\n",
    "\n",
    "    if 'shuffle' in kwargs:\n",
    "        _shuffle = kwargs.get('shuffle')\n",
    "    else:\n",
    "        _shuffle = True\n",
    "\n",
    "    return DataLoader(dataset, shuffle=_shuffle, batch_size=_batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cd79b09",
   "metadata": {},
   "source": [
    "#### Stats and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9bb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations_file_path    = '/home/dldou/Projets_ML/Unet/classes.csv'\n",
    "# train_dataset_img_path   = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/XTrain/'\n",
    "# train_dataset_label_path = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/yTrain/'\n",
    "\n",
    "# train_dataset            = TrayDataset(annotations_file_path, \n",
    "#                                        train_dataset_img_path, train_dataset_label_path\n",
    "#                                        )\n",
    "\n",
    "# # before any modification\n",
    "# classes = pd.read_csv(annotations_file_path)\n",
    "# print(classes.head())\n",
    "# print(\"Number of classes:\", len(classes))\n",
    "\n",
    "# # the panda dataframe of TrayDataset\n",
    "# train_dataset.classes_dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf12a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images\n",
    "#train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3e22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "#train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e52f5ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_dataset_in_pickle(dataset, save_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in dataset:\n",
    "        images.append(np.array(image))\n",
    "        labels.append(np.array(label))\n",
    "    images_path = save_path + 'images.pkl'\n",
    "    labels_path = save_path + 'labels.pkl'\n",
    "    with open(images_path, \"wb\") as images_file, open(labels_path, \"wb\") as labels_file:\n",
    "        pkl.dump(images, images_file)\n",
    "        pkl.dump(labels, labels_file)\n",
    "\n",
    "\n",
    "def get_stats_on_raw_numpy_data(images_pkl_path, labels_pkl_path, classes):\n",
    "    \"\"\"\n",
    "        Function that regroups the statistics and various plots necessary to uexplore the properties of the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(images_pkl_path, 'rb') as images_file, open(labels_pkl_path, \"rb\") as labels_file:\n",
    "        images = pkl.load(images_file)\n",
    "        labels = pkl.load(labels_file)\n",
    "\n",
    "    print(\"\\nTrain dataset information\\n\")\n",
    "    print(f\"Number of images:\\t{len(images)}\")\n",
    "    print(f\"Images' shape:\\t\\t{images[0].shape}\")\n",
    "    print(f\"Images' dytpe:\\t\\t{images[0].dtype}\")\n",
    "    print(f\"Labels' shape:\\t\\t{labels[0].shape}\")\n",
    "    print(f\"Labels' dytpe:\\t\\t{labels[0].dtype}\")\n",
    "    print(f\"Number of classes:\\t{len(classes)}\")\n",
    "\n",
    "\n",
    "    imgs_array = np.array(images)\n",
    "\n",
    "    # compute mean values of images for each channel    \n",
    "    mean_images_red = np.mean(imgs_array[:,:,:,0]) \n",
    "    mean_images_green = np.mean(imgs_array[:,:,1]) \n",
    "    mean_images_blue = np.mean(imgs_array[:,:,2]) \n",
    "    print(f\"Mean of RGB values:\\tred = {mean_images_red:.1f}, green = {mean_images_green:.1f}, blue = {mean_images_blue:.1f}\")\n",
    "\n",
    "    median_images_red = np.median(imgs_array[:,:,0]) \n",
    "    median_images_green = np.median(imgs_array[:,:,1]) \n",
    "    median_images_blue = np.median(imgs_array[:,:,2]) \n",
    "    print(f\"Median of RGB values:\\tred = {median_images_red:.1f}, green = {median_images_green:.1f}, blue = {median_images_blue:.1f}\")\n",
    "\n",
    "    # min = 0 --> black\n",
    "    min_images_red = np.min(imgs_array[:,:,:,0]) \n",
    "    min_images_green = np.min(imgs_array[:,:,1]) \n",
    "    min_images_blue = np.min(imgs_array[:,:,2]) \n",
    "    print(f\"Min of RGB values:\\tred = {min_images_red:.1f}, green = {min_images_green:.1f}, blue = {min_images_blue:.1f}\")\n",
    "\n",
    "    max_images_red = np.max(imgs_array[:,:,:,0]) \n",
    "    max_images_green = np.max(imgs_array[:,:,1]) \n",
    "    max_images_blue = np.max(imgs_array[:,:,2]) \n",
    "    print(f\"Max of RGB values:\\tred = {max_images_red:.1f}, green = {max_images_green:.1f}, blue = {max_images_blue:.1f}\")\n",
    "\n",
    "    std_images_red = np.std(imgs_array[:,:,0]) \n",
    "    std_images_green = np.std(imgs_array[:,:,1]) \n",
    "    std_images_blue = np.std(imgs_array[:,:,2]) \n",
    "    print(f\"Std of RGB values:\\tred = {std_images_red:.1f}, green = {std_images_green:.1f}, blue = {std_images_blue:.1f}\")\n",
    "\n",
    "    # fig to plot the repartition of pixels intensity in all the images\n",
    "    fig1 = plt.figure(figsize=(20,10))\n",
    "    channels_label = [\"R channel\", \"G channel\", \"B channel\"]\n",
    "    for channel in range(3):\n",
    "        ax = plt.subplot(2,2,channel+1)\n",
    "        pix_intensities = np.zeros(256)\n",
    "        for intensity_value in range(256):\n",
    "            pix_intensities[intensity_value] += np.count_nonzero(imgs_array[:,:,channel] == intensity_value)\n",
    "            ax.scatter(intensity_value, pix_intensities[intensity_value])\n",
    "            ax.set_title(channels_label[channel])\n",
    "        ax.set_xlabel(\"Intensity of pixels\")\n",
    "        ax.set_ylabel(\"Number of pixels\")\n",
    "        ax.grid()\n",
    "    plt.show()\n",
    "\n",
    "    classes_frequency = np.zeros((len(images), len(classes)))\n",
    "    image_idx = 0\n",
    "\n",
    "    for label in labels:\n",
    "        for class_idx in range(42):\n",
    "            classes_frequency[image_idx,class_idx] += np.count_nonzero(label == class_idx) \n",
    "        image_idx += 1\n",
    "    global_class_freq = np.sum(classes_frequency, axis=0) / (labels[0].shape[0] * labels[0].shape[1] * labels[0].shape[2] * len(images) )\n",
    "    \n",
    "    # plot the frequency of the classes\n",
    "    fig2, axs = plt.subplots(1,1)\n",
    "    fig2.set_figheight(5)\n",
    "    fig2.set_figwidth(10)\n",
    "    fig2.suptitle(\"Frequency of classes\")\n",
    "    axs.scatter(np.arange(1,44), global_class_freq)\n",
    "    axs.set_xticks(np.arange(1,44,2))\n",
    "    axs.set_xlabel(\"Index of class\")\n",
    "    axs.set_title(\"Global frequency of each classes\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    fig3 = plt.figure(figsize=(22,33))\n",
    "    fig3.suptitle(f\"Repartition of classes in the {len(images)} images of the dataset\")\n",
    "    for class_idx in range(len(classes)):\n",
    "        ax = plt.subplot(9,5,class_idx+1)\n",
    "        ax.hist(classes_frequency[:,class_idx] )\n",
    "        # convert y ticks to pourcentage of images\n",
    "        y_ticks = ax.get_yticks()\n",
    "        yticks = [int((tick / 1241) * 100) for tick in y_ticks]\n",
    "        if int(yticks[-1]) > 100:\n",
    "            yticks[-1] = 100\n",
    "        ax.set_yticklabels(yticks)\n",
    "        ax.set_ylabel('(%) of images')\n",
    "        ax.set_title(f\"class {classes['_class'].iloc[class_idx]}\")\n",
    "        ax.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return global_class_freq\n",
    "\n",
    "def get_mean_std_ToTensor(images_pkl_path, labels_pkl_path, plot=True):\n",
    "    \"\"\"\n",
    "        Function to obtain the mean and std of a dataset for each channels \n",
    "        after a ToTensor transform with scaling\n",
    "    \"\"\"\n",
    "    with open(images_pkl_path, 'rb') as images_file:\n",
    "        images = pkl.load(images_file)\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    images_tens = []\n",
    "    for image, label in images:\n",
    "        images_tens.append(transform(image))\n",
    "    images_tens = torch.stack(images_tens)\n",
    "\n",
    "    mean = torch.mean(images_tens, dim=(0,2,3))\n",
    "    std = torch.std(images_tens, dim=(0,2,3))\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def get_stats_on_tensor_data(images_pkl_path, labels_pkl_path, classes):\n",
    "    \"\"\"\n",
    "        Used to obtain statistical information on the dataset after torch transforms are applied on the data\n",
    "    \"\"\"\n",
    "    with open(images_pkl_path, 'rb') as images_file, open(labels_pkl_path, \"rb\") as labels_file:\n",
    "        images = pkl.load(images_file)\n",
    "        labels = pkl.load(labels_file)\n",
    "\n",
    "    to_tens_transform = transforms.ToTensor()\n",
    "    scal_images = []\n",
    "    for image in images:\n",
    "        to_tens_image = to_tens_transform(image)\n",
    "        scal_images.append(to_tens_image)\n",
    "    scal_images_tens = torch.stack(scal_images)\n",
    "    mean, std = torch.mean(scal_images_tens, dim=(0,2,3)), torch.std(scal_images_tens, dim=(0,2,3))\n",
    "    #print(mean)\n",
    "    #print(std)\n",
    "    \n",
    "    norm_transform = transforms.Normalize([0.10750454664230347, 0.09672276675701141, 0.07581707090139389], [0.22638827562332153, 0.2054644525051117, 0.16790764033794403])\n",
    "    scal_norm_images = []\n",
    "    for scal_image in scal_images:\n",
    "        scal_norm_images.append(norm_transform(scal_image))\n",
    "    scal_norm_images_tens = torch.stack(scal_norm_images)\n",
    "    new_mean, new_std = torch.mean(scal_norm_images_tens, dim=(0,2,3)), torch.std(scal_norm_images_tens, dim=(0,2,3))\n",
    "    #print(new_mean, new_std)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    # histogram after ToTensor\n",
    "    axs[0].hist(scal_images_tens.numpy().ravel(), bins=256)\n",
    "    axs[0].set_xlim(left=0, right=1)\n",
    "    axs[0].set_title(\"After ToTensor\")\n",
    "\n",
    "    axs[1].hist(scal_norm_images_tens.numpy().ravel(), bins=256)\n",
    "    axs[1].set_xlim(left=-2, right=2)\n",
    "    axs[1].set_title(\"After ToTensor and Normalize\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d52bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations_file_path    = '/home/dldou/Projets_ML/Unet/classes.csv'\n",
    "# classes = pd.read_csv(annotations_file_path)\n",
    "\n",
    "# get_stats_on_tensor_data(\"./saving/stats_images.pkl\", \"./saving/stats_labels.pkl\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d916e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = train_dataset\n",
    "#dump_dataset_in_pickle(train_dataset, save_path=\"./saving/stats_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f60a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_class_freq_numpy = get_stats_on_raw_numpy_data(images_pkl_path=\"./saving/stats_images.pkl\", labels_pkl_path=\"./saving/stats_labels.pkl\", classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e72dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency array to get the frequency of each classes and the new undetermined one\n",
    "def compute_new_frequency_array(freq_array, first_class_idx_of_the_rest):\n",
    "    \"\"\"\n",
    "        return a new frequency array containing the frequency of each classes and the new undetermined one\n",
    "    \"\"\"\n",
    "    # rescaling the values in the range [0, 1]\n",
    "    sum = np.sum(freq_array)\n",
    "    freq_array = freq_array / sum\n",
    "\n",
    "    # filling new frequency array with the computed frequencies\n",
    "    new_freq_array = np.zeros(len(freq_array[:first_class_idx_of_the_rest]) + 1)\n",
    "    for i in range(first_class_idx_of_the_rest):\n",
    "        new_freq_array[i] += freq_array[i]\n",
    "    new_freq_array[-1] += np.sum(freq_array[first_class_idx_of_the_rest:]) \n",
    "\n",
    "    if np.sum(new_freq_array) != 1.:\n",
    "        print(f'Total sum of the new frequency array: {np.sum(new_freq_array)}')\n",
    "\n",
    "    return new_freq_array\n",
    "\n",
    "#new_global_freq_array = compute_new_frequency_array(global_class_freq_numpy, 3)\n",
    "#print(new_global_freq_array)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2888e2d8",
   "metadata": {},
   "source": [
    "### Preprocessing of the data\n",
    "\n",
    "**Pipeline:**\n",
    "> 1. function to change the classes of index to create the undetermined class \n",
    "> 2. function to creates a panda dataframe that handle the new subset\n",
    "> 3. transform for data augmentation --> **OK**\n",
    "> 4. transform to convert to tensor + scaling and normalization --> **OK**\n",
    "> 5. function to obtain the new classes' frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26cd1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations_file_path    = '/home/dldou/Projets_ML/Unet/classes.csv'\n",
    "# train_dataset_img_path   = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/XTrain/'\n",
    "# train_dataset_label_path = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/yTrain/'\n",
    "\n",
    "# pad_scal_norm_transform = transforms.Compose([transforms.Pad((78,158), fill=0, padding_mode='constant'),\n",
    "#                                               transforms.ToTensor(), \n",
    "#                                               transforms.Normalize([0.10750454664230347, 0.09672276675701141, 0.07581707090139389], \n",
    "#                                                                    [0.22638827562332153, 0.2054644525051117, 0.16790764033794403])\n",
    "#                                              ])\n",
    "                                            \n",
    "\n",
    "# scal_norm_transform = transforms.Compose([transforms.ToTensor(),\n",
    "#                                           transforms.Normalize([0.3302825093269348, 0.29715806245803833, 0.23293016850948334], \n",
    "#                                                                [0.2896188199520111, 0.26483339071273804, 0.2236514538526535])\n",
    "#                                         ])\n",
    "# _data_augmentation = True\n",
    "# _random_rot_transform = None\n",
    "# _adapt_to_task = True\n",
    "# _first_class_idx_of_the_rest = 3\n",
    "\n",
    "# train_dataset = TrayDataset(annotations_file_path, train_dataset_img_path, train_dataset_label_path,\n",
    "#                             scal_norm_transform=scal_norm_transform, random_rot_transform=None, mask_transform=MaskPILToTensor(),\n",
    "#                             data_augmentation=_data_augmentation, \n",
    "#                             adapt_to_task=_adapt_to_task, first_class_idx_of_the_rest=_first_class_idx_of_the_rest)\n",
    "\n",
    "# #mean, std = train_dataset.get_mean_std_images()\n",
    "# #print(mean, std)\n",
    "\n",
    "# classes_frequency = train_dataset.get_classes_frequency()\n",
    "# print(\"Repartition of the class:\", classes_frequency)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb7229d8-5a44-4874-87bb-bd0edb30fab6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model\n",
    "\n",
    "Our model is a basic U-Net (**REFERENCE**). We made it tunable in order to control the depth, add batch normalization or dropout on the layers and choose between two activation functions: ReLU and LeakyReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc02b204-53e7-4257-a51f-31d47772b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as functionnal\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3635a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Double_conv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        \"\"\"\n",
    "            kwargs: - batchnorm (bool), add classic 2D batchnorm\n",
    "                    - dropout (bool), add dropout only if dropout_val also specified\n",
    "                    - dropout_val (float), between 0 and 1 specify the dropout value on all dropout layers\n",
    "                    - leaky_relu (bool), choose Leaky ReLU as activation function (alpha set by default)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        # layers\n",
    "        self.conv_x2      = nn.Sequential()\n",
    "        prev_channels     = self.in_channels\n",
    "\n",
    "        for i in range(2):\n",
    "            self.conv_x2.append(nn.Conv2d(prev_channels, self.out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "            if 'batchnorm' in kwargs and kwargs.get('batchnorm'):\n",
    "                self.conv_x2.append(nn.BatchNorm2d(self.out_channels))\n",
    "\n",
    "            if 'dropout' in kwargs and kwargs.get('dropout') and 'dropout_val' in kwargs:\n",
    "                self.conv_x2.append(nn.Dropout2d(p=kwargs.get('dropout_val'), inplace=True))\n",
    "\n",
    "            # activation\n",
    "            if 'leaky_relu' in kwargs and kwargs.get('leaky_relu'):\n",
    "                self.conv_x2.append(nn.LeakyReLU(inplace=True))\n",
    "            else:\n",
    "                self.conv_x2.append(nn.ReLU(inplace=True))\n",
    "\n",
    "            prev_channels = self.out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_x2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f2201f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block_down_unet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.block = nn.Sequential (\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            Double_conv(self.in_channels, self.out_channels, **kwargs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67496e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block_up_unet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.upsampling   = nn.ConvTranspose2d(self.in_channels, out_channels = self.in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv_x2      = Double_conv(self.in_channels, self.out_channels, **kwargs)\n",
    "\n",
    "    def forward(self, x, concat_tensor):\n",
    "        x = self.upsampling(x)\n",
    "        # handling the shape to make shortcut path possible\n",
    "        diffH = concat_tensor.size()[2] - x.size()[2]\n",
    "        diffW = concat_tensor.size()[3] - x.size()[3]\n",
    "\n",
    "        x = functionnal.pad(x, [diffW // 2, diffW - diffW // 2,\n",
    "                                diffH // 2, diffH - diffH // 2])\n",
    "        x_res = torch.cat([concat_tensor, x], dim=1)\n",
    "\n",
    "        return self.conv_x2(x_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5554898e-9e66-4de0-a21f-69f290eb0dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in_channels, n_out_channels, depth, **kwargs):\n",
    "        print(\"Initializing U-Net...\")\n",
    "        super().__init__()\n",
    "        self.n_in_channels     = n_in_channels\n",
    "        self.n_out_channels    = n_out_channels\n",
    "\n",
    "        # model building\n",
    "        self.mid_channels_list = [32 * 2**i for i in range(depth + 1)]\n",
    "        self.down_part         = nn.ModuleList()\n",
    "        self.up_part           = nn.ModuleList()\n",
    "\n",
    "        # first and last layer\n",
    "        self.input_layer  = Double_conv(self.n_in_channels, self.mid_channels_list[0], **kwargs)\n",
    "        self.output_layer = Double_conv(self.mid_channels_list[0], self.n_out_channels, **kwargs) \n",
    "\n",
    "        # Encoder path\n",
    "        for i in range(depth):\n",
    "            self.down_part.append(\n",
    "                Block_down_unet(self.mid_channels_list[i], self.mid_channels_list[i + 1], **kwargs)\n",
    "            )\n",
    "        # Up path\n",
    "        for i in reversed(range(depth)):\n",
    "            self.up_part.append(\n",
    "                Block_up_unet(self.mid_channels_list[i + 1], self.mid_channels_list[i], **kwargs)\n",
    "            )\n",
    "        print(\"Init U-Net done.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_block_outputs = []\n",
    "        \n",
    "        # First layer\n",
    "        x = self.input_layer(x)\n",
    "        down_block_outputs.append(x)\n",
    "        \n",
    "        # Encoder path\n",
    "        for i, down_layer in enumerate(self.down_part):\n",
    "            x = down_layer(x)\n",
    "            down_block_outputs.append(x)            \n",
    "        \n",
    "        # Decoder path with res path\n",
    "        for i, up_layer in enumerate(self.up_part, start=1):\n",
    "            x = up_layer(x, down_block_outputs[-i - 1])\n",
    "        \n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343ce38-4ea0-448b-9ff7-ab5b36f35100",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46f23d0b-09f3-4301-89a2-dd2fd927818b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d2c074a-a8bf-4d05-bcdf-42c8e64c4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, save_model_file_path):\n",
    "    \"\"\"\n",
    "        Function to save model's parameters\n",
    "    \"\"\"\n",
    "    torch.save(copy.deepcopy(model.state_dict()), save_model_file_path)\n",
    "\n",
    "\n",
    "def load_model(model, model_file_path):\n",
    "    \"\"\"\n",
    "        Function to load model's parameters\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(model_file_path))\n",
    "    \n",
    "\n",
    "def plot_results(model, test_loader):\n",
    "    \"\"\"\n",
    "    Display few images and segmentations\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axs = plt.subplots(3,3,figsize=(12,12))\n",
    "\n",
    "    #Display 4*4=16 images\n",
    "    for row_idx in range(3):\n",
    "\n",
    "        #Select a random image in the dataset\n",
    "        nof_images = len(test_loader.dataset)\n",
    "        idx = random.randrange(nof_images)\n",
    "\n",
    "        for data in test_loader:\n",
    "\n",
    "            #Inference\n",
    "            image, target = data[0].to('cuda:0'), data[1].to('cuda:0')\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(image)\n",
    "\n",
    "            break\n",
    "\n",
    "        #Sent back the image to the CPU + layout\n",
    "        image     = image[0,:,:,:].permute(1,2,0).to('cpu').numpy() + 0.5\n",
    "        target    = target[0,:,:,:].permute(1,2,0).to('cpu').numpy()\n",
    "        # Getting all classes on 1 image\n",
    "        output    = output[0,:,:,:].permute(1,2,0).to('cpu')\n",
    "        _, output = torch.max(output, dim=2)\n",
    "        output    = output.numpy()\n",
    "\n",
    "        #Plot\n",
    "        axs[row_idx, 0].imshow(image, vmin=-1, vmax=1)\n",
    "        axs[row_idx, 0].set_title(\"Image\")\n",
    "        axs[row_idx, 0].axis('off')\n",
    "\n",
    "        axs[row_idx, 1].imshow(target, cmap='gray', vmin=0, vmax=42)\n",
    "        axs[row_idx, 1].set_title(\"Ground truth segmentation\")\n",
    "        axs[row_idx, 1].axis('off')\n",
    "\n",
    "\n",
    "        axs[row_idx, 2].imshow(output, cmap='gray', vmin=0, vmax=1)\n",
    "        axs[row_idx, 2].set_title(\"Net segmentation\")\n",
    "        axs[row_idx, 2].axis('off')\n",
    "    \n",
    "    fig.suptitle(\"{} on few examples\\n Reached accuracy with 3 epochs: {}%\".format(model.__class__.__name__, 0.02325))\n",
    "\n",
    "    #Save\n",
    "    plt.savefig(str(model.__class__.__name__) + \"_accuraccy_0.02325\" + \".pdf\")\n",
    "\n",
    "    #Show\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_performances(file_path, \n",
    "                            model_name, optimizer_name, loss_name, \n",
    "                            learning_rate, \n",
    "                            save=True):\n",
    "\n",
    "    results = pkl.load(open(file_path, 'rb'))\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(1,2, sharex=True, figsize=(15,5))\n",
    "\n",
    "    l_train_epoch_loss = list()\n",
    "    l_epoch_accuracy   = list()\n",
    "\n",
    "    for epoch, train_epoch_loss, epoch_accuracy in results:\n",
    "        l_train_epoch_loss.append(train_epoch_loss)\n",
    "        l_epoch_accuracy.append(epoch_accuracy.to('cpu'))\n",
    "\n",
    "    ax0.set_title(\"Evolution of the loss\")\n",
    "    ax0.plot([i+1 for i in range(len(l_train_epoch_loss))], l_train_epoch_loss)\n",
    "    ax0.set_xticks([i+1 for i in range(len(l_train_epoch_loss))])\n",
    "    ax0.set_xlabel(\"Epoch index\")\n",
    "    ax0.set_ylabel(\"Loss\")\n",
    "    ax0.grid()\n",
    "    \n",
    "    ax1.set_title(\"Evolution of the accuracy\")\n",
    "    ax1.plot([i+1 for i in range(len(l_epoch_accuracy))], l_epoch_accuracy)\n",
    "    ax1.set_xticks([i+1 for i in range(len(l_epoch_accuracy))])\n",
    "    ax1.set_xlabel(\"Epoch index\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.grid()\n",
    "\n",
    "    suptitle = \"Performances of \" + model_name + \" with \" + \"optimizer \" + optimizer_name + \" (lr=\" + str(learning_rate) + \")\" + \", \" + \"loss \" + loss_name \n",
    "    fig.suptitle(suptitle)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(suptitle + \".pdf\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db5064a4",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19334f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from torchmetrics.classification import MulticlassJaccardIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ff40d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, device):\n",
    "         self.device = device\n",
    "\n",
    "\n",
    "    def train_step(self, train_dataloader, model, criterion, optimizer):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, masks in train_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                masks  = masks[:,0,:,:].long().to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "        mean_train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "        return mean_train_loss\n",
    "    \n",
    "\n",
    "    def valid_step(self, valid_dataloader, model, criterion, metric):\n",
    "         \n",
    "        model.eval()\n",
    "        running_valid_loss = 0.0\n",
    "        accuracy         = 0.0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, masks in valid_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                masks  = masks[:,0,:,:].long().to(self.device)\n",
    "\n",
    "                outputs = model(images)\n",
    "\n",
    "                # compute the predicted class for each pixels from the logits output of the model\n",
    "                predicted_class = torch.max(outputs, dim=1).indices\n",
    "                predicted_class = predicted_class.int()\n",
    "                valid_loss = criterion(outputs, masks)\n",
    "\n",
    "                running_valid_loss += valid_loss.item()\n",
    "                # Compute the Jaccard Multiclass index (sum by defalut)\n",
    "                accuracy += metric(predicted_class, masks).item()\n",
    "        mean_valid_loss = running_valid_loss / len(valid_dataloader)\n",
    "        mean_accuracy = accuracy / len(valid_dataloader)\n",
    "\n",
    "        return mean_valid_loss, mean_accuracy\n",
    "         \n",
    "    \n",
    "    def train(self, n_epochs, n_splits, kfold_random_state,\n",
    "              train_dataset, batch_size, \n",
    "              model, criterion, optimizer, metric,\n",
    "              train_loss_name, test_loss_name, accuracy_name,\n",
    "              save_model_file_path, results_file_path,\n",
    "              best_accuracy_is_maximal=False\n",
    "              ):\n",
    "        \"\"\"\n",
    "            Main entry point training loop\n",
    "        \"\"\"\n",
    "        print(\"The model will be running on\", self.device, \"device\")\n",
    "        model  = model.to(self.device)\n",
    "        metric = metric.to(self.device)\n",
    "\n",
    "        history = {'train_loss': [], 'test_loss': [], 'test_acc':[]}\n",
    "\n",
    "        # cross validation (random state for reproducibility)\n",
    "        splits = KFold(n_splits=n_splits,shuffle=True,random_state=kfold_random_state)\n",
    "\n",
    "        for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
    "\n",
    "            print('Fold {}'.format(fold + 1))\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_idx)\n",
    "            valid_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                 \n",
    "                 #Training\n",
    "                epoch_train_loss = self.train_step(train_loader, model, criterion, optimizer)\n",
    "                #Validation\n",
    "                epoch_valid_loss, epoch_accuracy = self.valid_step(valid_loader, model, criterion, metric)\n",
    "            \n",
    "                print(f'Epoch: {epoch}, {train_loss_name}: {epoch_train_loss:.2f}, {test_loss_name}: {epoch_valid_loss:.2f}, {accuracy_name}: {epoch_accuracy:.2f}')\n",
    "                \n",
    "                #Save model when best accuracy is beaten\n",
    "                #########################################\n",
    "                # if best_accuracy_is_maximal:\n",
    "                #     if epoch_accuracy > best_accuracy:\n",
    "                #         save_epoch_path = save_epoch_path\n",
    "                #         self.save_model(save_epoch_path)\n",
    "                #         best_accuracy = epoch_accuracy\n",
    "                # else:\n",
    "                #     if epoch_accuracy < best_accuracy:\n",
    "                #         save_epoch_path = save_epoch_path\n",
    "                #         self.save_model(save_epoch_path)\n",
    "                #         best_accuracy = epoch_accuracy\n",
    "\n",
    "                history['train_loss'].append(epoch_train_loss)\n",
    "                history['test_loss'].append(epoch_valid_loss)\n",
    "                history['test_acc'].append(epoch_accuracy)         \n",
    "\n",
    "        # Saving the model\n",
    "        print('Saving the model...\\n')\n",
    "        model = model.to('cpu')\n",
    "        ####################################################\n",
    "        save_model(model, save_model_file_path)\n",
    "\n",
    "        # Saving the performances\n",
    "        with open(results_file_path, 'wb') as f:\n",
    "            pkl.dump(history, f) \n",
    "\n",
    "        print(\"Training finish.\\n\") \n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c832d9-6526-4cc0-84a1-818abaee6b25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f646cef2",
   "metadata": {},
   "source": [
    "#### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40224c3a-9e35-4cf4-a320-2b07136e288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    ### Data\n",
    "    annotations_file_path    = '/home/dldou/Projets_ML/Unet/classes.csv'\n",
    "    train_dataset_img_path   = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/XTrain/'\n",
    "    train_dataset_label_path = '/home/dldou/Projets_ML/Unet/TrayDataset/TrayDataset/yTrain/'\n",
    "\n",
    "    pad_scal_norm_transform = transforms.Compose([transforms.Pad((78,158), fill=0, padding_mode='constant'),\n",
    "                                                transforms.ToTensor(), \n",
    "                                                transforms.Normalize([0.10750454664230347, 0.09672276675701141, 0.07581707090139389], \n",
    "                                                                    [0.22638827562332153, 0.2054644525051117, 0.16790764033794403])\n",
    "                                                ])                                   \n",
    "\n",
    "    scal_norm_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.3302825093269348, 0.29715806245803833, 0.23293016850948334], \n",
    "                                                                [0.2896188199520111, 0.26483339071273804, 0.2236514538526535])\n",
    "                                            ])\n",
    "    _data_augmentation    = True\n",
    "    _random_rot_transform = None\n",
    "    _adapt_to_task        = True\n",
    "    _first_class_idx_of_the_rest = 3\n",
    "\n",
    "    train_dataset = TrayDataset(annotations_file_path, train_dataset_img_path, train_dataset_label_path,\n",
    "                                scal_norm_transform=scal_norm_transform, random_rot_transform=None, mask_transform=MaskPILToTensor(),\n",
    "                                data_augmentation=_data_augmentation, \n",
    "                                adapt_to_task=_adapt_to_task, first_class_idx_of_the_rest=_first_class_idx_of_the_rest)\n",
    "\n",
    "    #mean, std = train_dataset.get_mean_std_images()\n",
    "    #print(mean, std)\n",
    "\n",
    "    classes_frequency = train_dataset.get_classes_frequency()\n",
    "    print(\"Repartition of classes:\", classes_frequency)\n",
    "    \n",
    "\n",
    "    ### Model \n",
    "    n_in_channels = 3\n",
    "    n_classes     = 4\n",
    "    depth         = 5\n",
    "    unet = U_Net(n_in_channels=n_in_channels, n_out_channels=n_classes, depth=depth)\n",
    "    summary(unet, (1, 3, 572, 572))\n",
    "\n",
    "    ### Trainer \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trainer = Trainer(device)\n",
    "\n",
    "    # Training\n",
    "    n_epochs             = 5\n",
    "    n_splits             = 2\n",
    "    kfold_random_state   = 42\n",
    "    batch_size           = 3\n",
    "    model                = unet\n",
    "    criterion            = nn.CrossEntropyLoss()\n",
    "    lr                   = 1e-2\n",
    "    optimizer            = torch.optim.Adam(model.parameters(), lr)\n",
    "    metric               = MulticlassJaccardIndex(num_classes=n_classes)\n",
    "    train_loss_name      = \"Cross Entropy Loss 2D\"\n",
    "    test_loss_name       = \"Cross Entropy Loss 2D\"\n",
    "    accuracy_name        = \"Multiclasses Jaccard score\"\n",
    "    save_model_file_path = \"./saving/trained_Unet_model.pth\"\n",
    "    results_file_path    = \"./saving/results.pkl\"\n",
    "\n",
    "    # training loop\n",
    "    model = trainer.train(n_epochs, n_splits, kfold_random_state,\n",
    "                          train_dataset, batch_size, \n",
    "                          model, criterion, optimizer, metric,\n",
    "                          train_loss_name, test_loss_name, accuracy_name,\n",
    "                          save_model_file_path, results_file_path,\n",
    "                          best_accuracy_is_maximal=False\n",
    "                          )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "623098ef",
   "metadata": {},
   "source": [
    "#### Running main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "396a90eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of TrayDataset...\n",
      "Adapting the labels and dataframe to task...\n",
      "Adapt TrayDataset to the task done.\n",
      "Init TrayDataset done.\n",
      "[-1.9499249503951432e-07, -2.255097228953673e-07, -6.354454171741963e-07] [1.0, 1.0, 1.0]\n",
      "Repartition of classes: tensor([0.5463, 0.3243, 0.0233, 0.1062])\n",
      "Initializing U-Net...\n",
      "Init U-Net done.\n",
      "The model will be running on cuda device\n",
      "Fold 1\n",
      "Epoch: 0, Cross Entropy Loss 2D: 72847218.76, Cross Entropy Loss 2D: 1.37, Multiclasses Jaccard score: 0.08\n",
      "Epoch: 1, Cross Entropy Loss 2D: 1.37, Cross Entropy Loss 2D: 1.37, Multiclasses Jaccard score: 0.08\n",
      "Epoch: 2, Cross Entropy Loss 2D: 1.37, Cross Entropy Loss 2D: 1.37, Multiclasses Jaccard score: 0.09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39474/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_39474/1143191011.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     model = trainer.train(n_epochs, n_splits, kfold_random_state,\n\u001b[0m\u001b[1;32m     64\u001b[0m                           \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_39474/4276456719.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, n_splits, kfold_random_state, train_dataset, batch_size, model, criterion, optimizer, metric, train_loss_name, test_loss_name, accuracy_name, save_model_file_path, results_file_path, best_accuracy_is_maximal)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                  \u001b[0;31m#Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;31m#Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mepoch_valid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_39474/4276456719.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, train_dataloader, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m#Update parameters of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             adam(params_with_grad,\n\u001b[0m\u001b[1;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;31m# update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mstep_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f7eefa5",
   "metadata": {},
   "source": [
    "## Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = trainer.model.to('cuda:0')\n",
    "# plot_results(model, test_dataloader)\n",
    "\n",
    "# file_path      = \"results.pkl\"\n",
    "# model_name     = \"U-Net\"\n",
    "# optimizer_name = \"Adam\"\n",
    "# loss_name      = \"2D Cross Entropy\"\n",
    "# learning_rate  = 1e-4\n",
    "# save           = True\n",
    "# plot_model_performances(file_path, \n",
    "#                         model_name, optimizer_name, loss_name, \n",
    "#                         learning_rate, \n",
    "#                         save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1b4e034",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As we can see, our model assigns the background class to all pixels after only one epoch. This is due to the 2D cross entropy cost function which isn't adapted to a dataset where the classes are unbalanced. We'll try the Focal Cross Entropy Loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8cc5b7cf7ab45a4eb9c5d10fcde61976ab495d4e3d71a8f87de6440d779c3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
